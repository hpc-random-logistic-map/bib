@INPROCEEDINGS{olivier, 
author={Olivier, S. and Prins, J.}, 
booktitle={Parallel Processing, 2008. ICPP '08. 37th International Conference on}, 
title={Scalable Dynamic Load Balancing Using UPC}, 
year={2008}, 
month={Sept}, 
pages={123-131}, 
keywords={parallel programming;resource allocation;tree data structures;tree searching;UPC;Unified Parallel C;asynchronous work-stealing implementation;available work dissemination;distributed memory;scalable dynamic load balancing;shared memory;synthetic tree-structured search space;termination detection cost;unbalanced tree search;Aerodynamics;Computer science;Cost function;Delay;Hardware;Load management;Parallel processing;Protocols;Read-write memory;State-space methods;Unbalanced Tree Search benchmark;Unified Parallel C;distributed memory systems;load balancing;performance analysis;unbalanced parallel computations;work stealing}, 
doi={10.1109/ICPP.2008.19}, 
ISSN={0190-3918},
annote={Olivier and Prins implement an asynchronous work-stealing dynamic load balancer with Unified Parallel C (UPC). They evaluate the performance of their balancer with the Unbalanced Tree Search (UTS) benchmark, which is a synthetic tree-structured search space that is highly imbalanced. They observe parallel efficiency of 80\% using 1024 processors performing over 85,000 total load balancing operations per second continuously. An additional finding is that the careful use of one sided reads and writes is necessary to minimize the communication overhead. The authors' findings indicate that we should minimize the number of read and write operations as we compute solutions to the fixed point equations in order to keep the communication overhad low.}
}

@article{dlb,
    author    = {Marc H. Willibeek-LeMair and Anthony P. Reeves},
    title     = {Strategies for Dynamic Load Balancing on Highly Parallel Computers},
    journal   = {IEEE Transactions on Parallel and Distributed Computing},
    volume    = {4},
    month     = {September},
    number    = {4},
    year      = {1993},
    annote    = {Willibeek-LeMair and Reeves discuss five strategies for dynamic load balancing: sender initiated diffusion, receiver initiated diffusion, hierarchical balancing model, gradient model, and dimension exchange method. The authors consider tasks such as:  processor load evaluation, load balancing profitability, task migration, and task selection. Given the trade off between accuracy and increased time for communication, they conclude that the receiver initiated diffusion (RID) is the best method for dynamically load balancing. It is the method that scales the best with the number of processors and requires the least amount of communication overhead. Dynamic load balancing is key for solving systems whose solutions are defined recursively, such as a fixed point iteration. We will use the findings of this paper to guide our program design such that it is the best suited for RID.}
}

@article{lamb,
    author    = {Jeroen S.W. Lamb and Martin Rasmussen and Christian S. Rodrigues},
    title     = {TOPOLOGICAL BIFURCATIONS OF MINIMAL INVARIANT SETS FOR SET-VALUED DYNAMICAL SYSTEMS},
    journal   = {Proceedings of the American Mathematical Society},
    year      = {2013},
    annote    = {Lamb, et. al explore the concept of set-valued bifurcations as an extension of the more common single-valued bifurcation. The kind of problems the authors are interested in are random dynamical systems, such as the Random Logistic Map. This paper serves as a theoretical underpinning for our simulation, and also as a reference for the set-valued bifurcation diagram we plan to generate.}
}

@article{athreya,
year={2000},
journal={Journal of Theoretical Probability},
volume={13},
number={2},
title={Random Logistic Maps.},
publisher={Kluwer Academic Publishers-Plenum Publishers},
keywords={random logistic maps; invariant measure},
author={Athreya, K.B. and Dai, Jack},
pages={595-608},
annote={Athreya and Dai explore the concept of a time-varying logistic map. In a sense, they lay out the general groundwork for exploring a spatially-varying logistic map. They find a theoretical explanation for their observations, expressed as probabilities. These findings imply that (and this is beyond the scope of the project) there could be some interesting observations from the spatially-random logistic map, and the observations can be quantified as probabilities. This project's goal is to characterize the map in terms of the average number of period $p$ orbits in any given realization, and also to find the set-valued bifurcation diagram that describes this system.}
}


@online{openmpi:faq,
author = {{Various contributers: The Open MPI Project}},
title = {{Open MPI: Open Source High Performance Computing}},
year = {2014},
url = {http://www.open-mpi.org/faq/},
subtitle = {FAQ: Running MPI jobs},
OPTtitleaddon = {•},
OPTlanguage = {•},
OPTversion = {•},
note = {http://www.open-mpi.org/faq/},
organization = {{The Open MPI Project}},
OPTdate = {•},
OPTmonth = {•},
OPTaddendum = {•},
OPTpubstate = {•},
urldate = {17 November 2014},
annote = {Explanatory paragraph goes here.}
}

@online{openmpi:buffalo,
author = {{Center for Computational Research}},
title = {{MPI} and Parallel Computing},
year = {2004--2014},
url = {http://www.buffalo.edu/ccr/support/UserGuide/AdvancedTopics/mpi.html},
subtitle = {FAQ: Running MPI jobs},
OPTtitleaddon = {•},
OPTlanguage = {•},
OPTversion = {•},
note = {http://www.buffalo.edu/ccr/support/UserGuide/AdvancedTopics/mpi.html},
OPTorganization = {•},
OPTdate = {•},
OPTmonth = {•},
OPTaddendum = {•},
OPTpubstate = {•},
urldate = {17 November 2014},
annote = {Explanatory paragraph goes here.}
}

@book{chandra2001parallel,
  title={Parallel programming in OpenMP},
  author={Chandra, Rohit},
  year={2001},
  publisher={Morgan Kaufmann}
 annote = {The authors of this book were originally SGI engineers who were involved in the design and implementation of OpenMP. The main information avaible about OpenMPI can be found at www.openmp.org. Although full specification of OpenMP is appropriate and complete, it is not a very accessible format for programmers wishing to use OpenMP for developing parallel application. This book tries to fullfill the needs of these programmers. This book can serve as a complete reference guild, also can be the mean tool for exploring options to improve performance on the OpenMP section of the project(removing dependency, load balancing between threads).}
}

@book{sato2010beyond,
  title={Beyond Loop Level Parallelism in OpenMP: Accelerators, Tasking and More},
  author={Sato, Mitsuhisa and Hanawa, Toshihiro and M{\"u}ller, Matthias S and Chapman, Barbara and de Supinski, Bronis R},
  volume={6132},
  year={2010},
  publisher={Springer}
 annote = {Chapter one of this book, Enabling Low-Overhead Hybrid MPI/OpenMP Parallelism with MPC, introduce a new module to MPC framework handling a fully 2.5-compliant OpenMP runtime completely intergrated to an MPI1.3 implementation. This chapter review strategy how to target oversubscribing capabilities and the possibility to run hybrid MPI/OpenMP application with a limited overhead. This can be really useful for us when implementing our strategy as we are most likely will face the same problem, ie how to optimize performance when implement the hybrid system using limited sharing resources.}
}










